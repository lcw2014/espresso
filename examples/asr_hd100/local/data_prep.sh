#!/bin/bash

# 2020 Hanyang Univ
# lullaby0804@hanyang.ac.kr

token_size=$1
multi_unit_tokenizing=$2 

train=data/ttrain
test=data/test

echo ============================================================================
echo "Data & Lexicon & Language Preparation                                     "
echo ============================================================================

data_dir=data

#input_data_dir1=$data_dir/Address
#input_data_dir2=$data_dir/Car05_clean
#input_data_dir3=$data_dir/CleanSentence01_nangdok2
#input_data_dir4=$data_dir/Dict01
#input_data_dir5=$data_dir/Dict02
#input_data_dir6=$data_dir/Etri_2011_pc
#input_data_dir7=$data_dir/Etri_2011_phone
#input_data_dir8=$data_dir/Etri_Clean_free_2ch
#input_data_dir9=$data_dir/Etri_Clean_free_8ch
#input_data_dir10=$data_dir/Etri_Clean_free_mid
#input_data_dir11=$data_dir/Etri_Clean_nangdok_8ch
#input_data_dir12=$data_dir/Etri_Clean_nangdok_mid
#input_data_dir13=$data_dir/Etri_Clean_word_2ch
#input_data_dir14=$data_dir/Etri_Clean_word_8ch
#input_data_dir15=$data_dir/Etri_Clean_word_mid
#input_data_dir16=$data_dir/hyundai_izen
#input_data_dir17=$data_dir/KsponSpeech
#input_data_dir18=$data_dir/PBS_SpeechDB
#input_data_dir19=$data_dir/pluto
#input_data_dir20=$data_dir/Seoul_nangdok
#input_data_dir21=$data_dir/SiTEC_Word
#input_data_dir22=$data_dir/Zeroth
input_data_dir23=$data_dir/HD_100h

target_data_dir=data/Combined_data

utils/combine_data.sh $target_data_dir $input_data_dir1 $input_data_dir2 $input_data_dir3 $input_data_dir4 $input_data_dir5 $input_data_dir6 $input_data_dir7 $input_data_dir8 $input_data_dir9 $input_data_dir10 $input_data_dir11 $input_data_dir12 $input_data_dir13 $input_data_dir14 $input_data_dir15 $input_data_dir16 $input_data_dir17 $input_data_dir18 $input_data_dir19 $input_data_dir20 $input_data_dir21 $input_data_dir22 $input_data_dir23

if [ $token_size -ne 0 ]; then
  local/multi_unit_tokenizing.sh $target_data_dir $token_size $multi_unit_tokenizing
fi

#cat $target_data_dir/text | awk '{ print substr($0, index($0,$2)) }' > $target_data_dir/text.txt
cat $target_data_dir/text | awk '{ print $1 }' > tmp
paste tmp $target_data_dir/text > tmp2
cat tmp2 | sed -e s/'\t'/' '/g | cut -d " " -f 3- > $target_data_dir/text.txt
rm tmp
rm tmp2

#local/gen_lexicon.sh $data_dir $target_data_dir
local/gen_lexicon_HD.sh $data_dir $target_data_dir

dir=data/local
lmtmp=$dir/lm
dict=$dir/dict_tmp

mkdir -p $lmtmp
mkdir -p $dict

tr -d '\r' < $data_dir/lexicon.txt > $dict/lexicon.txt
cp $target_data_dir/text.txt $lmtmp/text.txt

num_lexicon1=`cat $dict/lexicon.txt | wc -l`
num_lexicon2=`expr $num_lexicon1 - 3`

cat $dict/lexicon.txt | tail -$num_lexicon2 > $lmtmp/librispeech-lexicon.txt
cat $lmtmp/librispeech-lexicon.txt | awk '{ print $1 }' > $lmtmp/librispeech-vocab.txt

# Get lm.arpa from text.txt
./../../../espresso/tools/kaldi/tools/srilm/bin/i686-m64/ngram-count -text $lmtmp/text.txt -write-vocab $lmtmp/training.vocab -write1 $lmtmp/unigram.freq
./../../../espresso/tools/kaldi/tools/srilm/bin/i686-m64/ngram-count -vocab $lmtmp/training.vocab -text $lmtmp/text.txt -order 3 -write $lmtmp/training.count
./../../../espresso/tools/kaldi/tools/srilm/bin/i686-m64/ngram-count -vocab $lmtmp/training.vocab -read $lmtmp/training.count -order 3 -lm $lmtmp/lm.arpa

# We will gonna use lm generated by paired data
cp $lmtmp/lm.arpa $lmtmp/lm_tglarge.arpa
cp $lmtmp/lm.arpa $lmtmp/lm_tgmed.arpa
cp $lmtmp/lm.arpa $lmtmp/lm_tgsmall.arpa

gzip -f $lmtmp/lm_tglarge.arpa
gzip -f $lmtmp/lm_tgmed.arpa
gzip -f $lmtmp/lm_tgsmall.arpa

# Get phone lists...
grep -v -w sil $dict/lexicon.txt | \
  awk '{for(n=2;n<=NF;n++) { p[$n]=1; }} END{for(x in p) {print x}}' | \
  sort | uniq > $dict/nonsilence_phones.txt

echo sil > $dict/silence_phones.txt
echo sil > $dict/optional_silence.txt
touch $dict/extra_questions.txt

# Prepare data
utils/subset_data_dir_tr_cv.sh --cv-spk-percent 8 $target_data_dir \
  data/train_clean_5 data/dev_clean_2 || exit 1;

cp $data_dir/words.txt $lmtmp/words.txt
echo "data_prep succeeded."
